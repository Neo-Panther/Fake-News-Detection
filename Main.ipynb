{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# get the GPU device name\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('Data/train.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])\n",
    "test_data = pd.read_table('Data/test.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])\n",
    "valid_data = pd.read_table('Data/valid.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"Testing Data Info:\")\n",
    "print(test_data.info())\n",
    "print(\"Validation Data Info:\")\n",
    "print(valid_data.info())\n",
    "print(train_data.label.unique())\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Data to Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on degree of truthfullness\n",
    "y_label_dict = {\"pants-fire\" : 0, \"false\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"true\" : 5}\n",
    "\n",
    "train_data['output'] = train_data['label'].apply(lambda i: y_label_dict[i])\n",
    "valid_data['output'] = valid_data['label'].apply(lambda i: y_label_dict[i])\n",
    "test_data['output'] = test_data['label'].apply(lambda i: y_label_dict[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top speakers to consider a parameter\n",
    "no_speaker = 25\n",
    "# based on the frequency of the label (only consider the top no_speaker speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_speakers = train_data['speaker'].value_counts().reset_index()[:no_speaker].to_dict()['speaker']\n",
    "frequent_speakers = dict((v, k) for k, v in frequent_speakers.items())\n",
    "print(frequent_speakers)\n",
    "\n",
    "def convert_speaker_to_num(speaker):\n",
    "  # speaker not in the top 20, assign it to the 21st category\n",
    "  other = no_speaker\n",
    "  if isinstance(speaker, str):\n",
    "    if speaker in frequent_speakers:\n",
    "      return frequent_speakers[speaker]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['speaker_id'] = train_data['speaker'].apply(convert_speaker_to_num)\n",
    "valid_data['speaker_id'] = valid_data['speaker'].apply(convert_speaker_to_num)\n",
    "test_data['speaker_id'] = test_data['speaker'].apply(convert_speaker_to_num)\n",
    "train_data['speaker_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top jobs to consider a parameter\n",
    "no_jobs = 25\n",
    "# based on the frequency of the label (only consider the top no_jobs speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_jobs = train_data['job'].value_counts().reset_index()[:no_jobs].to_dict()['job']\n",
    "frequent_jobs = dict((v, k) for k, v in frequent_jobs.items())\n",
    "print(frequent_jobs)\n",
    "\n",
    "def convert_job_to_num(job):\n",
    "  # job not in the top jobs, assign it to the last category\n",
    "  other = no_jobs\n",
    "  if isinstance(job, str):\n",
    "    if job in frequent_jobs:\n",
    "      return frequent_jobs[job]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['job_id'] = train_data['job'].apply(convert_job_to_num)\n",
    "valid_data['job_id'] = valid_data['job'].apply(convert_job_to_num)\n",
    "test_data['job_id'] = test_data['job'].apply(convert_job_to_num)\n",
    "train_data['job_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top parties to consider a parameter\n",
    "no_party = 9\n",
    "# based on the frequency of the label (only consider the top no_party speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_party = train_data['party'].value_counts().reset_index()[:no_party].to_dict()['party']\n",
    "frequent_party = dict((v, k) for k, v in frequent_party.items())\n",
    "print(frequent_party)\n",
    "\n",
    "def convert_party_to_num(party):\n",
    "  # party not in the top parties, assign it to the last category\n",
    "  other = no_party\n",
    "  if isinstance(party, str):\n",
    "    if party in frequent_party:\n",
    "      return frequent_party[party]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['party_id'] = train_data['party'].apply(convert_party_to_num)\n",
    "valid_data['party_id'] = valid_data['party'].apply(convert_party_to_num)\n",
    "test_data['party_id'] = test_data['party'].apply(convert_party_to_num)\n",
    "train_data['party_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top states to consider a parameter\n",
    "no_state = 30\n",
    "# based on the frequency of the label (only consider the top no_state speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_state = train_data['state'].value_counts().reset_index()[:no_state].to_dict()['state']\n",
    "frequent_state = dict((v, k) for k, v in frequent_state.items())\n",
    "print(frequent_state)\n",
    "\n",
    "def convert_state_to_num(state):\n",
    "  # state not in the top states, assign it to the last category\n",
    "  other = no_state\n",
    "  if isinstance(state, str):\n",
    "    if state in frequent_state:\n",
    "      return frequent_state[state]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['state_id'] = train_data['state'].apply(convert_state_to_num)\n",
    "valid_data['state_id'] = valid_data['state'].apply(convert_state_to_num)\n",
    "test_data['state_id'] = test_data['state'].apply(convert_state_to_num)\n",
    "train_data['state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top subjects to consider a parameter\n",
    "no_subject = 30\n",
    "# based on the frequency of the label (only consider the top no_subject speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_subject = train_data['subject'].value_counts().reset_index()[:no_subject].to_dict()['subject']\n",
    "frequent_subject = dict((v, k) for k, v in frequent_subject.items())\n",
    "print(frequent_subject)\n",
    "\n",
    "def convert_subject_to_num(subject):\n",
    "  # subject not in the top subjects, assign it to the last category\n",
    "  other = no_subject\n",
    "  if isinstance(subject, str):\n",
    "    if subject in frequent_subject:\n",
    "      return frequent_subject[subject]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['subject_id'] = train_data['subject'].apply(convert_subject_to_num)\n",
    "valid_data['subject_id'] = valid_data['subject'].apply(convert_subject_to_num)\n",
    "test_data['subject_id'] = test_data['subject'].apply(convert_subject_to_num)\n",
    "train_data['subject_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of top venues to consider a parameter\n",
    "no_venue = 30\n",
    "# based on the frequency of the label (only consider the top no_venue speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_venue = train_data['venue'].value_counts().reset_index()[:no_venue].to_dict()['venue']\n",
    "frequent_venue = dict((v, k) for k, v in frequent_venue.items())\n",
    "print(frequent_venue)\n",
    "\n",
    "def convert_venue_to_num(venue):\n",
    "  # venue not in the top venues, assign it to the last category\n",
    "  other = no_venue\n",
    "  if isinstance(venue, str):\n",
    "    if venue in frequent_venue:\n",
    "      return frequent_venue[venue]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['venue_id'] = train_data['venue'].apply(convert_venue_to_num)\n",
    "valid_data['venue_id'] = valid_data['venue'].apply(convert_venue_to_num)\n",
    "test_data['venue_id'] = test_data['venue'].apply(convert_venue_to_num)\n",
    "train_data['venue_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "if not os.path.exists('vocab_dict.pkl'):\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "  tokenizer.fit_on_texts(train_data['statement'])\n",
    "  vocab_dict = tokenizer.word_index\n",
    "  pickle.dump(vocab_dict, open('vocab_dict.pkl', 'wb'))\n",
    "else:\n",
    "  vocab_dict = pickle.load(open('vocab_dict.pkl', 'rb'))\n",
    "\n",
    "def convert_statement_to_vec(statement):\n",
    "  stmnt = ''.join(word for word in statement.split() if word not in stopwords.words('english'))\n",
    "  text = tf.keras.preprocessing.text.text_to_word_sequence(stmnt)\n",
    "  return [vocab_dict[word] for word in text if word in vocab_dict]\n",
    "\n",
    "train_data['statement_freq'] = train_data['statement'].apply(convert_statement_to_vec)\n",
    "valid_data['statement_freq'] = valid_data['statement'].apply(convert_statement_to_vec)\n",
    "test_data['statement_freq'] = test_data['statement'].apply(convert_statement_to_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pos_tags = {'ADJ': 'adjective', 'ADP': 'adposition', 'ADV': 'adverb',\n",
    "            'AUX': 'auxiliary verb', 'CONJ': 'coordinating conjunction',\n",
    "            'DET': 'determiner', 'INTJ': 'interjection', 'NOUN': 'noun',\n",
    "            'NUM': 'numeral', 'PART': 'particle', 'PRON': 'pronoun',\n",
    "            'PROPN': 'proper noun', 'PUNCT': 'punctuation', 'X': 'other',\n",
    "            'SCONJ': 'subord conjunction', 'SYM': 'symbol', 'VERB': 'verb'}\n",
    "\"\"\"\n",
    "# create a dictionary to convert the pos tags to numbers, arbitrary\n",
    "pos_dict = {'NOUN' : 0, 'VERB' : 1, 'ADP' : 2, 'PROPN' : 3, 'PUNCT' : 4,\n",
    "            'DET' : 5, 'ADJ' : 6, 'NUM' : 7, 'ADV' : 8, 'PRON' : 9}\n",
    "other = len(pos_dict.values())  # fpr all other pos tags\n",
    "\n",
    "def convert_sentence_to_pos(sentence: str):\n",
    "  doc = nlp(sentence)\n",
    "  return [pos_dict.get(token.pos_, other) for token in doc]\n",
    "\n",
    "train_data['statement_pos'] = train_data['statement'].apply(convert_sentence_to_pos)\n",
    "valid_data['statement_pos'] = valid_data['statement'].apply(convert_sentence_to_pos)\n",
    "test_data['statement_pos'] = test_data['statement'].apply(convert_sentence_to_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all dependencies:\n",
    "dep_dict = {'ACL' : 0, 'ACOMP' : 1, 'ADVCL' : 2, 'ADVMOD' : 3, 'AGENT' : 4,\n",
    "            'AMOD' : 5, 'APPOS' : 6, 'ATTR' : 7, 'AUX' : 8, 'AUXPASS' : 9,\n",
    "            'CASE' : 10, 'CC' : 11, 'CCOMP' : 12, 'COMPOUND' : 13, 'CONJ' : 14,\n",
    "            'CSUBJ' : 15, 'CSUBJPASS' : 16, 'DATIVE' : 17, 'DEP' : 18,\n",
    "            'DET' : 19, 'DOBJ' : 20, 'EXPL' : 21, 'INTJ' : 22, 'MARK' : 23,\n",
    "            'META' : 24, 'NEG' : 25, 'NOUNMOD' : 26, 'NPMOD' : 27, 'NSUBJ' : 28,\n",
    "            'NSUBJPASS' : 29, 'NUMMOD' : 30, 'OPRD' : 31, 'PARATAXIS' : 32,\n",
    "            'PCOMP' : 33, 'POBJ' : 34, 'POSS' : 35, 'PRECONJ' : 36, 'PREDET' : 37,\n",
    "            'PREP' : 38, 'PRT' : 39, 'PUNCT' : 40, 'QUANTMOD' : 41,\n",
    "            'RELCL' : 42, 'ROOT' : 43, 'XCOMP' : 44}\n",
    "\"\"\"\n",
    "# create a dictionary to convert the dep tags to numbers, arbitrary\n",
    "dep_dict = {'punct' : 0, 'prep' : 1, 'pobj' : 2, 'compound' : 3, 'det' : 4,\n",
    "            'nsubj' : 5, 'ROOT' : 6, 'amod' : 7, 'dobj' : 8, 'aux' : 9}\n",
    "other = len(dep_dict.values())  # for all other dep tags\n",
    "\n",
    "def convert_sentence_to_dep(sentence):\n",
    "  doc = nlp(sentence)\n",
    "  return [dep_dict.get(token.dep_, other) for token in doc]\n",
    "\n",
    "train_data['statement_dep'] = train_data['statement'].apply(convert_sentence_to_dep)\n",
    "valid_data['statement_dep'] = valid_data['statement'].apply(convert_sentence_to_dep)\n",
    "test_data['statement_dep'] = test_data['statement'].apply(convert_sentence_to_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "We use the pretrained GloVe embeddings to convertwords into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "embeddings = {}\n",
    "word = ''\n",
    "try:\n",
    "  with open('glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0].lower()\n",
    "      embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "except FileNotFoundError:\n",
    "  print('File glove.6B.100d.txt was not found in this directory')\n",
    "  print('Get the file from the references provided in README.md')\n",
    "  raise FileNotFoundError\n",
    "print(len(embeddings), \": Embeddings loaded\")\n",
    "print(embed_dim, \": Embedding dimension\")\n",
    "\n",
    "num_words = len(vocab_dict) + 1\n",
    "embed_matrix = np.zeros((num_words, embed_dim))\n",
    "for word, i in vocab_dict.items():\n",
    "  embed_vector = embeddings.get(word)\n",
    "  if embed_vector is not None:\n",
    "    embed_matrix[i] = embed_vector\n",
    "\n",
    "pos_embeddings = np.identity(max(pos_dict.values())+1, dtype=int)\n",
    "dep_embeddings = np.identity(max(dep_dict.values())+1, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab_dict.keys())\n",
    "lstm_size = 100\n",
    "num_steps = 15\n",
    "num_epochs = 30\n",
    "batch_size = 40\n",
    "\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,3,3]\n",
    "filter_size = 128\n",
    "\n",
    "#Meta data related hyper params\n",
    "num_party = len(train_data.party_id.unique())\n",
    "num_state = len(train_data.state_id.unique())\n",
    "num_venue = len(train_data.venue_id.unique())\n",
    "num_job = len(train_data.job_id.unique())\n",
    "num_sub = len(train_data.subject_id.unique())\n",
    "num_speaker = len(train_data.speaker_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sentence Info (Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['statement_freq']\n",
    "X_val = valid_data['statement_freq']\n",
    "X_test = test_data['statement_freq']\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(train_data['output'], num_classes=6)\n",
    "Y_val = tf.keras.utils.to_categorical(valid_data['output'], num_classes=6)\n",
    "Y_test = list(test_data['output'])\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "X_train_pos = train_data['statement_pos']\n",
    "X_val_pos = valid_data['statement_pos']\n",
    "X_test_pos = test_data['statement_pos']\n",
    "\n",
    "X_train_pos = tf.keras.preprocessing.sequence.pad_sequences(X_train_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val_pos = tf.keras.preprocessing.sequence.pad_sequences(X_val_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test_pos = tf.keras.preprocessing.sequence.pad_sequences(X_test_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "X_train_dep = train_data['statement_dep']\n",
    "X_val_dep = valid_data['statement_dep']\n",
    "X_test_dep = test_data['statement_dep']\n",
    "\n",
    "X_train_dep = tf.keras.preprocessing.sequence.pad_sequences(X_train_dep, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val_dep = tf.keras.preprocessing.sequence.pad_sequences(X_val_dep, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test_dep = tf.keras.preprocessing.sequence.pad_sequences(X_test_dep, maxlen=num_steps, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_train = tf.keras.utils.to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "party_val = tf.keras.utils.to_categorical(valid_data['party_id'], num_classes=num_party)\n",
    "party_test = tf.keras.utils.to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "\n",
    "state_train = tf.keras.utils.to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "state_val = tf.keras.utils.to_categorical(valid_data['state_id'], num_classes=num_state)\n",
    "state_test = tf.keras.utils.to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "\n",
    "venue_train = tf.keras.utils.to_categorical(train_data['venue_id'], num_classes=num_venue)\n",
    "venue_val = tf.keras.utils.to_categorical(valid_data['venue_id'], num_classes=num_venue)\n",
    "venue_test = tf.keras.utils.to_categorical(test_data['venue_id'], num_classes=num_venue)\n",
    "\n",
    "job_train = tf.keras.utils.to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "job_val = tf.keras.utils.to_categorical(valid_data['job_id'], num_classes=num_job)\n",
    "job_test = tf.keras.utils.to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "\n",
    "subject_train = tf.keras.utils.to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "subject_val = tf.keras.utils.to_categorical(valid_data['subject_id'], num_classes=num_sub)\n",
    "subject_test = tf.keras.utils.to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "\n",
    "speaker_train = tf.keras.utils.to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "speaker_val = tf.keras.utils.to_categorical(valid_data['speaker_id'], num_classes=num_speaker)\n",
    "speaker_test = tf.keras.utils.to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "X_train_meta = np.hstack((party_train, state_train, venue_train, job_train, subject_train, speaker_train))\n",
    "X_val_meta = np.hstack((party_val, state_val, venue_val, job_val, subject_val, speaker_val))\n",
    "X_test_meta = np.hstack((party_test, state_test, venue_test, job_test, subject_test, speaker_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Matrix Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_meta.shape, X_val_meta.shape, X_test_meta.shape)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_val.shape)\n",
    "print(X_train_pos.shape, X_val_pos.shape, X_test_pos.shape)\n",
    "print(X_train_dep.shape, X_val_dep.shape, X_test_dep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: tf.keras.models.Model, model_file_name: str, use_pos = False, use_meta = False, use_dep = False):\n",
    "  sgd = tf.keras.optimizers.SGD(learning_rate=0.025, clipvalue=0.3, nesterov=True)\n",
    "  # adam = tf.keras.optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "  tb = tf.keras.callbacks.TensorBoard()\n",
    "  csv_logger = tf.keras.callbacks.CSVLogger('train.log')\n",
    "  filepath = model_file_name + '_weights.keras'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "  train_input = [X_train]\n",
    "  valid_input = [X_val]\n",
    "  if use_pos:\n",
    "    train_input.append(X_train_pos)\n",
    "    valid_input.append(X_val_pos)\n",
    "  if use_dep:\n",
    "    train_input.append(X_train_dep)\n",
    "    valid_input.append(X_val_dep)\n",
    "  if use_meta:\n",
    "    train_input.append(X_train_meta)\n",
    "    valid_input.append(X_val_meta)\n",
    "  model.fit(train_input, [Y_train], epochs=num_epochs, batch_size=batch_size, validation_data=(valid_input, [Y_val]), callbacks=[tb, csv_logger, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_file_name: str, use_pos = False, use_meta = False, use_dep = False):\n",
    "  model: tf.keras.models.Model = tf.keras.models.load_model(model_file_name + '_weights.keras')\n",
    "  input = [X_test]\n",
    "  if use_pos:\n",
    "    input.append(X_test_pos)\n",
    "  if use_dep:\n",
    "    input.append(X_test_dep)\n",
    "  if use_meta:\n",
    "    input.append(X_test_meta)\n",
    "  predictions = model.predict(input, batch_size=batch_size, verbose=1)\n",
    "  n = len(predictions)\n",
    "\n",
    "  tp = tn = fp = fn = 0\n",
    "  for p in range(n):\n",
    "    if np.argmax(predictions[p]) == Y_test[p]:\n",
    "      if Y_test[p]  >= 3:\n",
    "        tp += 1\n",
    "      else:\n",
    "        tn += 1\n",
    "    else:\n",
    "      if Y_test[p] >= 3:\n",
    "        fn += 1\n",
    "      else:\n",
    "        fp += 1\n",
    "  print(n == len(Y_test))\n",
    "  correct = np.sum(np.argmax(predictions, axis=1) == Y_test)\n",
    "  print(\"Correctly predicted: \", correct, \"out of\", n)\n",
    "  print(\"Accuracy: \", correct*100/n)\n",
    "  pickle.dump(predictions, open(model_file_name + '_predictions.pkl', 'wb'))\n",
    "\n",
    "  print(\"True Positive: \", tp)\n",
    "  print(\"True Negative: \", tn)\n",
    "  print(\"False Positive: \", fp)\n",
    "  print(\"False Negative: \", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pos = False\n",
    "use_meta = True\n",
    "use_dep = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x_stmt = tf.keras.layers.Embedding(vocab_length+1, embed_dim, weights=[embed_matrix], trainable=False)(statement_input)\n",
    "\n",
    "pos_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "x_pos = tf.keras.layers.Embedding(max(pos_dict.values())+1, max(pos_dict.values())+1, weights=[pos_embeddings], trainable=False)(pos_input)\n",
    "\n",
    "dep_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "x_dep = tf.keras.layers.Embedding(max(dep_dict.values())+1, max(dep_dict.values())+1, weights=[dep_embeddings], trainable=False)(dep_input)\n",
    "\n",
    "meta_input = tf.keras.layers.Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = tf.keras.layers.Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "kernel_stmt = []\n",
    "kernel_pos = []\n",
    "kernel_dep = []\n",
    "for kernel in kernel_sizes:\n",
    "  x_1 = tf.keras.layers.Conv1D(filter_size, kernel)(x_stmt)\n",
    "  x_1 = tf.keras.layers.GlobalMaxPooling1D()(x_1)\n",
    "  kernel_stmt.append(x_1)\n",
    "\n",
    "  x_2 = tf.keras.layers.Conv1D(filter_size, kernel)(x_pos)\n",
    "  x_2 = tf.keras.layers.GlobalMaxPooling1D()(x_2)\n",
    "  kernel_pos.append(x_2)\n",
    "\n",
    "  x_3 = tf.keras.layers.Conv1D(filter_size, kernel)(x_dep)\n",
    "  x_3 = tf.keras.layers.GlobalMaxPooling1D()(x_3)\n",
    "  kernel_dep.append(x_3)\n",
    "\n",
    "conv_in1 = tf.keras.layers.concatenate(kernel_stmt)\n",
    "conv_in1 = tf.keras.layers.Dropout(0.6)(conv_in1)\n",
    "conv_in1 = tf.keras.layers.Dense(128, activation='relu')(conv_in1)\n",
    "\n",
    "conv_in2 = tf.keras.layers.concatenate(kernel_pos)\n",
    "conv_in2 = tf.keras.layers.Dropout(0.6)(conv_in2)\n",
    "conv_in2 = tf.keras.layers.Dense(128, activation='relu')(conv_in2)\n",
    "\n",
    "conv_in3 = tf.keras.layers.concatenate(kernel_dep)\n",
    "conv_in3 = tf.keras.layers.Dropout(0.6)(conv_in3)\n",
    "conv_in3 = tf.keras.layers.Dense(128, activation='relu')(conv_in3)\n",
    "\n",
    "lays = [conv_in1]\n",
    "if use_pos:\n",
    "  lays.append(conv_in2)\n",
    "if use_dep:\n",
    "  lays.append(conv_in3)\n",
    "if use_meta:\n",
    "  lays.append(x_meta)\n",
    "x = tf.keras.layers.concatenate(lays)\n",
    "\n",
    "main_output = tf.keras.layers.Dense(6, activation='softmax', name='main_output')(x)\n",
    "inputs = [statement_input]\n",
    "if use_pos:\n",
    "  inputs.append(pos_input)\n",
    "if use_dep:\n",
    "  inputs.append(dep_input)\n",
    "if use_meta:\n",
    "  inputs.append(meta_input)\n",
    "model_cnn = tf.keras.models.Model(inputs=inputs, outputs=[main_output])\n",
    "print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = tf.keras.models.Sequential()\n",
    "hidden_size = embed_dim\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_length+1, hidden_size))\n",
    "model_lstm.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_size)))\n",
    "model_lstm.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "statement_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x_stmt = tf.keras.layers.Embedding(vocab_length+1, embed_dim, weights=[embed_matrix], trainable=False)(statement_input)\n",
    "lstm_in = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_stmt)\n",
    "\n",
    "pos_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "x_pos = tf.keras.layers.Embedding(max(pos_dict.values())+1, max(pos_dict.values())+1, weights=[pos_embeddings], trainable=False)(pos_input)\n",
    "lstm_in2 = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_pos)\n",
    "\n",
    "dep_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "x_dep = tf.keras.layers.Embedding(max(dep_dict.values())+1, max(dep_dict.values())+1, weights=[dep_embeddings], trainable=False)(dep_input)\n",
    "lstm_in3 = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_dep)\n",
    "\n",
    "meta_input = tf.keras.layers.Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = tf.keras.layers.Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "lays = [lstm_in]\n",
    "if use_pos:\n",
    "  lays.append(lstm_in2)\n",
    "if use_dep:\n",
    "  lays.append(lstm_in3)\n",
    "if use_meta:\n",
    "  lays.append(x_meta)\n",
    "x = tf.keras.layers.concatenate(lays)\n",
    "\n",
    "main_output = tf.keras.layers.Dense(6, activation='softmax', name='main_output')(x)\n",
    "inputs = [statement_input]\n",
    "if use_pos:\n",
    "  inputs.append(pos_input)\n",
    "if use_dep:\n",
    "  inputs.append(dep_input)\n",
    "if use_meta:\n",
    "  inputs.append(meta_input)\n",
    "model_lstm = tf.keras.models.Model(inputs=inputs, outputs=[main_output])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_cnn, 'cnn', use_pos, use_meta, use_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_lstm,'lstm', use_pos, use_meta, use_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test('cnn', use_pos, use_meta, use_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test('lstm', use_pos, use_meta, use_dep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
