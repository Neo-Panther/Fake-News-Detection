{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 19:55:13.064240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 19:55:13.232069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732371913.292711    9318 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732371913.311674    9318 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 19:55:13.457053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/destrox/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# get the GPU device name\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('Data/train.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])\n",
    "test_data = pd.read_table('Data/test.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])\n",
    "valid_data = pd.read_table('Data/valid.tsv', names = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"venue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10240 entries, 0 to 10239\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           10240 non-null  object \n",
      " 1   label        10240 non-null  object \n",
      " 2   statement    10240 non-null  object \n",
      " 3   subject      10238 non-null  object \n",
      " 4   speaker      10238 non-null  object \n",
      " 5   job          7342 non-null   object \n",
      " 6   state        8030 non-null   object \n",
      " 7   party        10238 non-null  object \n",
      " 8   barely-true  10238 non-null  float64\n",
      " 9   false        10238 non-null  float64\n",
      " 10  half-true    10238 non-null  float64\n",
      " 11  mostly-true  10238 non-null  float64\n",
      " 12  pants-fire   10238 non-null  float64\n",
      " 13  venue        10138 non-null  object \n",
      "dtypes: float64(5), object(9)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "Testing Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1267 entries, 0 to 1266\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           1267 non-null   object\n",
      " 1   label        1267 non-null   object\n",
      " 2   statement    1267 non-null   object\n",
      " 3   subject      1267 non-null   object\n",
      " 4   speaker      1267 non-null   object\n",
      " 5   job          942 non-null    object\n",
      " 6   state        1005 non-null   object\n",
      " 7   party        1267 non-null   object\n",
      " 8   barely-true  1267 non-null   int64 \n",
      " 9   false        1267 non-null   int64 \n",
      " 10  half-true    1267 non-null   int64 \n",
      " 11  mostly-true  1267 non-null   int64 \n",
      " 12  pants-fire   1267 non-null   int64 \n",
      " 13  venue        1250 non-null   object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 138.7+ KB\n",
      "None\n",
      "Validation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1284 entries, 0 to 1283\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           1284 non-null   object\n",
      " 1   label        1284 non-null   object\n",
      " 2   statement    1284 non-null   object\n",
      " 3   subject      1284 non-null   object\n",
      " 4   speaker      1284 non-null   object\n",
      " 5   job          939 non-null    object\n",
      " 6   state        1005 non-null   object\n",
      " 7   party        1284 non-null   object\n",
      " 8   barely-true  1284 non-null   int64 \n",
      " 9   false        1284 non-null   int64 \n",
      " 10  half-true    1284 non-null   int64 \n",
      " 11  mostly-true  1284 non-null   int64 \n",
      " 12  pants-fire   1284 non-null   int64 \n",
      " 13  venue        1272 non-null   object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 140.6+ KB\n",
      "None\n",
      "['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']\n",
      "           id        label                                          statement  \\\n",
      "0   2635.json        false  Says the Annies List political group supports ...   \n",
      "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
      "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
      "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
      "\n",
      "                              subject         speaker                   job  \\\n",
      "0                            abortion    dwayne-bohac  State representative   \n",
      "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
      "2                      foreign-policy    barack-obama             President   \n",
      "3                         health-care    blog-posting                   NaN   \n",
      "4                        economy,jobs   charlie-crist                   NaN   \n",
      "\n",
      "      state       party  barely-true  false  half-true  mostly-true  \\\n",
      "0     Texas  republican          0.0    1.0        0.0          0.0   \n",
      "1  Virginia    democrat          0.0    0.0        1.0          1.0   \n",
      "2  Illinois    democrat         70.0   71.0      160.0        163.0   \n",
      "3       NaN        none          7.0   19.0        3.0          5.0   \n",
      "4   Florida    democrat         15.0    9.0       20.0         19.0   \n",
      "\n",
      "   pants-fire                venue  \n",
      "0         0.0             a mailer  \n",
      "1         0.0      a floor speech.  \n",
      "2         9.0               Denver  \n",
      "3        44.0       a news release  \n",
      "4         2.0  an interview on CNN  \n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"Testing Data Info:\")\n",
    "print(test_data.info())\n",
    "print(\"Validation Data Info:\")\n",
    "print(valid_data.info())\n",
    "print(train_data.label.unique())\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Data to Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on degree of truthfullness\n",
    "y_label_dict = {\"pants-fire\" : 0, \"false\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"true\" : 5}\n",
    "\n",
    "train_data['output'] = train_data['label'].apply(lambda i: y_label_dict[i])\n",
    "valid_data['output'] = valid_data['label'].apply(lambda i: y_label_dict[i])\n",
    "test_data['output'] = test_data['label'].apply(lambda i: y_label_dict[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barack-obama': 0, 'donald-trump': 1, 'hillary-clinton': 2, 'mitt-romney': 3, 'scott-walker': 4, 'john-mccain': 5, 'rick-perry': 6, 'chain-email': 7, 'marco-rubio': 8, 'rick-scott': 9, 'ted-cruz': 10, 'bernie-s': 11, 'chris-christie': 12, 'facebook-posts': 13, 'charlie-crist': 14, 'newt-gingrich': 15, 'jeb-bush': 16, 'joe-biden': 17, 'blog-posting': 18, 'paul-ryan': 19, 'sarah-palin': 20, 'john-boehner': 21, 'michele-bachmann': 22, 'rick-santorum': 23, 'national-republican-congressional-committee': 24}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "speaker_id\n",
       "25    7295\n",
       "0      488\n",
       "1      273\n",
       "2      239\n",
       "3      176\n",
       "4      149\n",
       "5      148\n",
       "7      142\n",
       "6      142\n",
       "8      117\n",
       "9      115\n",
       "10      93\n",
       "11      88\n",
       "13      78\n",
       "12      78\n",
       "14      70\n",
       "15      69\n",
       "17      63\n",
       "16      63\n",
       "18      59\n",
       "19      56\n",
       "20      52\n",
       "21      49\n",
       "24      46\n",
       "22      46\n",
       "23      46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top speakers to consider a parameter\n",
    "no_speaker = 25\n",
    "# based on the frequency of the label (only consider the top no_speaker speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_speakers = train_data['speaker'].value_counts().reset_index()[:no_speaker].to_dict()['speaker']\n",
    "frequent_speakers = dict((v, k) for k, v in frequent_speakers.items())\n",
    "print(frequent_speakers)\n",
    "\n",
    "def convert_speaker_to_num(speaker):\n",
    "  # speaker not in the top 20, assign it to the 21st category\n",
    "  other = no_speaker\n",
    "  if isinstance(speaker, str):\n",
    "    if speaker in frequent_speakers:\n",
    "      return frequent_speakers[speaker]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['speaker_id'] = train_data['speaker'].apply(convert_speaker_to_num)\n",
    "valid_data['speaker_id'] = valid_data['speaker'].apply(convert_speaker_to_num)\n",
    "test_data['speaker_id'] = test_data['speaker'].apply(convert_speaker_to_num)\n",
    "train_data['speaker_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'President': 0, 'U.S. Senator': 1, 'Governor': 2, 'President-Elect': 3, 'U.S. senator': 4, 'Presidential candidate': 5, 'Former governor': 6, 'U.S. Representative': 7, 'Milwaukee County Executive': 8, 'Senator': 9, 'State Senator': 10, 'U.S. representative': 11, 'U.S. House of Representatives': 12, 'Attorney': 13, 'Congressman': 14, 'Social media posting': 15, 'Governor of New Jersey': 16, 'Co-host on CNN\\'s \"Crossfire\"': 17, 'State Representative': 18, 'State representative': 19, 'U.S. Congressman': 20, 'Congresswoman': 21, 'Speaker of the House of Representatives': 22, 'State senator': 23, 'state representative': 24}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "job_id\n",
       "25    6348\n",
       "0      492\n",
       "1      479\n",
       "2      391\n",
       "3      273\n",
       "4      263\n",
       "5      254\n",
       "6      176\n",
       "7      172\n",
       "8      149\n",
       "9      147\n",
       "10     108\n",
       "11     103\n",
       "12     102\n",
       "13      81\n",
       "14      80\n",
       "15      78\n",
       "16      78\n",
       "17      73\n",
       "18      72\n",
       "19      66\n",
       "20      63\n",
       "22      50\n",
       "21      50\n",
       "23      48\n",
       "24      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top jobs to consider a parameter\n",
    "no_jobs = 25\n",
    "# based on the frequency of the label (only consider the top no_jobs speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_jobs = train_data['job'].value_counts().reset_index()[:no_jobs].to_dict()['job']\n",
    "frequent_jobs = dict((v, k) for k, v in frequent_jobs.items())\n",
    "print(frequent_jobs)\n",
    "\n",
    "def convert_job_to_num(job):\n",
    "  # job not in the top jobs, assign it to the last category\n",
    "  other = no_jobs\n",
    "  if isinstance(job, str):\n",
    "    if job in frequent_jobs:\n",
    "      return frequent_jobs[job]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['job_id'] = train_data['job'].apply(convert_job_to_num)\n",
    "valid_data['job_id'] = valid_data['job'].apply(convert_job_to_num)\n",
    "test_data['job_id'] = test_data['job'].apply(convert_job_to_num)\n",
    "train_data['job_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'republican': 0, 'democrat': 1, 'none': 2, 'organization': 3, 'independent': 4, 'newsmaker': 5, 'libertarian': 6, 'activist': 7, 'journalist': 8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "party_id\n",
       "0    4497\n",
       "1    3336\n",
       "2    1744\n",
       "3     219\n",
       "4     147\n",
       "9     124\n",
       "5      56\n",
       "6      40\n",
       "7      39\n",
       "8      38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top parties to consider a parameter\n",
    "no_party = 9\n",
    "# based on the frequency of the label (only consider the top no_party speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_party = train_data['party'].value_counts().reset_index()[:no_party].to_dict()['party']\n",
    "frequent_party = dict((v, k) for k, v in frequent_party.items())\n",
    "print(frequent_party)\n",
    "\n",
    "def convert_party_to_num(party):\n",
    "  # party not in the top parties, assign it to the last category\n",
    "  other = no_party\n",
    "  if isinstance(party, str):\n",
    "    if party in frequent_party:\n",
    "      return frequent_party[party]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['party_id'] = train_data['party'].apply(convert_party_to_num)\n",
    "valid_data['party_id'] = valid_data['party'].apply(convert_party_to_num)\n",
    "test_data['party_id'] = test_data['party'].apply(convert_party_to_num)\n",
    "train_data['party_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Texas': 0, 'Florida': 1, 'Wisconsin': 2, 'New York': 3, 'Illinois': 4, 'Ohio': 5, 'Georgia': 6, 'Virginia': 7, 'Rhode Island': 8, 'New Jersey': 9, 'Oregon': 10, 'Massachusetts': 11, 'Arizona': 12, 'California': 13, 'Washington, D.C.': 14, 'Vermont': 15, 'Pennsylvania': 16, 'New Hampshire': 17, 'Arkansas': 18, 'Tennessee': 19, 'Kentucky': 20, 'Maryland': 21, 'Delaware': 22, 'Alaska': 23, 'Minnesota': 24, 'North Carolina': 25, 'Nevada': 26, 'Indiana': 27, 'Missouri': 28, 'New Mexico': 29}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state_id\n",
       "30    2539\n",
       "0     1009\n",
       "1      997\n",
       "2      713\n",
       "3      657\n",
       "4      556\n",
       "5      447\n",
       "6      426\n",
       "7      407\n",
       "8      369\n",
       "9      241\n",
       "10     239\n",
       "11     206\n",
       "12     182\n",
       "13     159\n",
       "14     120\n",
       "15      98\n",
       "16      90\n",
       "17      86\n",
       "18      84\n",
       "19      75\n",
       "20      74\n",
       "21      69\n",
       "22      68\n",
       "23      65\n",
       "24      56\n",
       "25      56\n",
       "26      48\n",
       "27      38\n",
       "28      36\n",
       "29      30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top states to consider a parameter\n",
    "no_state = 30\n",
    "# based on the frequency of the label (only consider the top no_state speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_state = train_data['state'].value_counts().reset_index()[:no_state].to_dict()['state']\n",
    "frequent_state = dict((v, k) for k, v in frequent_state.items())\n",
    "print(frequent_state)\n",
    "\n",
    "def convert_state_to_num(state):\n",
    "  # state not in the top states, assign it to the last category\n",
    "  other = no_state\n",
    "  if isinstance(state, str):\n",
    "    if state in frequent_state:\n",
    "      return frequent_state[state]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['state_id'] = train_data['state'].apply(convert_state_to_num)\n",
    "valid_data['state_id'] = valid_data['state'].apply(convert_state_to_num)\n",
    "test_data['state_id'] = test_data['state'].apply(convert_state_to_num)\n",
    "train_data['state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'health-care': 0, 'taxes': 1, 'immigration': 2, 'elections': 3, 'education': 4, 'candidates-biography': 5, 'economy': 6, 'guns': 7, 'economy,jobs': 8, 'federal-budget': 9, 'jobs': 10, 'energy': 11, 'abortion': 12, 'foreign-policy': 13, 'state-budget': 14, 'education,state-budget': 15, 'transportation': 16, 'crime': 17, 'ethics': 18, 'iraq': 19, 'campaign-finance': 20, 'terrorism': 21, 'environment': 22, 'history': 23, 'job-accomplishments': 24, 'legal-issues': 25, 'social-security': 26, 'deficit,federal-budget': 27, 'state-budget,taxes': 28, 'energy,environment': 29}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subject_id\n",
       "30    6910\n",
       "0      381\n",
       "1      308\n",
       "2      253\n",
       "3      252\n",
       "4      237\n",
       "5      190\n",
       "6      137\n",
       "7      130\n",
       "8      125\n",
       "9      121\n",
       "10      98\n",
       "11      94\n",
       "12      92\n",
       "13      85\n",
       "14      75\n",
       "15      69\n",
       "16      64\n",
       "17      59\n",
       "18      58\n",
       "19      55\n",
       "20      53\n",
       "21      53\n",
       "22      52\n",
       "24      45\n",
       "23      45\n",
       "25      42\n",
       "27      40\n",
       "26      40\n",
       "28      39\n",
       "29      38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top subjects to consider a parameter\n",
    "no_subject = 30\n",
    "# based on the frequency of the label (only consider the top no_subject speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_subject = train_data['subject'].value_counts().reset_index()[:no_subject].to_dict()['subject']\n",
    "frequent_subject = dict((v, k) for k, v in frequent_subject.items())\n",
    "print(frequent_subject)\n",
    "\n",
    "def convert_subject_to_num(subject):\n",
    "  # subject not in the top subjects, assign it to the last category\n",
    "  other = no_subject\n",
    "  if isinstance(subject, str):\n",
    "    if subject in frequent_subject:\n",
    "      return frequent_subject[subject]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['subject_id'] = train_data['subject'].apply(convert_subject_to_num)\n",
    "valid_data['subject_id'] = valid_data['subject'].apply(convert_subject_to_num)\n",
    "test_data['subject_id'] = test_data['subject'].apply(convert_subject_to_num)\n",
    "train_data['subject_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a news release': 0, 'an interview': 1, 'a press release': 2, 'a speech': 3, 'a TV ad': 4, 'a tweet': 5, 'a campaign ad': 6, 'a television ad': 7, 'a radio interview': 8, 'a debate': 9, 'a news conference': 10, 'a Facebook post': 11, 'a campaign commercial': 12, 'a television interview': 13, 'a press conference': 14, 'a speech.': 15, 'a press release.': 16, 'a TV interview': 17, 'a radio ad': 18, 'a chain e-mail': 19, 'an interview on CNN': 20, 'a TV ad.': 21, 'a campaign mailer': 22, 'comments on ABC\\'s \"This Week\"': 23, 'an interview on Fox News': 24, 'an interview.': 25, 'a campaign TV ad': 26, 'a news release.': 27, 'a TV interview.': 28, 'an ad': 29}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "venue_id\n",
       "30    7569\n",
       "0      241\n",
       "1      229\n",
       "2      223\n",
       "3      214\n",
       "4      180\n",
       "5      156\n",
       "6      132\n",
       "7      123\n",
       "8      106\n",
       "9       92\n",
       "10      85\n",
       "11      74\n",
       "12      73\n",
       "13      68\n",
       "14      65\n",
       "15      57\n",
       "16      49\n",
       "17      47\n",
       "18      45\n",
       "19      41\n",
       "21      40\n",
       "20      40\n",
       "23      39\n",
       "22      39\n",
       "24      38\n",
       "25      37\n",
       "26      36\n",
       "27      35\n",
       "28      34\n",
       "29      33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take number of top venues to consider a parameter\n",
    "no_venue = 30\n",
    "# based on the frequency of the label (only consider the top no_venue speakers as relevent, after 20, rest have less than 50 data points, not relevent)\n",
    "frequent_venue = train_data['venue'].value_counts().reset_index()[:no_venue].to_dict()['venue']\n",
    "frequent_venue = dict((v, k) for k, v in frequent_venue.items())\n",
    "print(frequent_venue)\n",
    "\n",
    "def convert_venue_to_num(venue):\n",
    "  # venue not in the top venues, assign it to the last category\n",
    "  other = no_venue\n",
    "  if isinstance(venue, str):\n",
    "    if venue in frequent_venue:\n",
    "      return frequent_venue[venue]\n",
    "    else:\n",
    "      return other\n",
    "  else:\n",
    "    return other\n",
    "\n",
    "train_data['venue_id'] = train_data['venue'].apply(convert_venue_to_num)\n",
    "valid_data['venue_id'] = valid_data['venue'].apply(convert_venue_to_num)\n",
    "test_data['venue_id'] = test_data['venue'].apply(convert_venue_to_num)\n",
    "train_data['venue_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "if not os.path.exists('vocab_dict.pkl'):\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "  tokenizer.fit_on_texts(train_data['statement'])\n",
    "  vocab_dict = tokenizer.word_index\n",
    "  pickle.dump(vocab_dict, open('vocab_dict.pkl', 'wb'))\n",
    "else:\n",
    "  vocab_dict = pickle.load(open('vocab_dict.pkl', 'rb'))\n",
    "\n",
    "def convert_statement_to_vec(statement):\n",
    "  stmnt = ''.join(word for word in statement.split() if word not in stopwords.words('english'))\n",
    "  text = tf.keras.preprocessing.text.text_to_word_sequence(stmnt)\n",
    "  return [vocab_dict[word] for word in text if word in vocab_dict]\n",
    "\n",
    "train_data['statement_freq'] = train_data['statement'].apply(convert_statement_to_vec)\n",
    "valid_data['statement_freq'] = valid_data['statement'].apply(convert_statement_to_vec)\n",
    "test_data['statement_freq'] = test_data['statement'].apply(convert_statement_to_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pos_tags = {'ADJ': 'adjective', 'ADP': 'adposition', 'ADV': 'adverb',\n",
    "            'AUX': 'auxiliary verb', 'CONJ': 'coordinating conjunction',\n",
    "            'DET': 'determiner', 'INTJ': 'interjection', 'NOUN': 'noun',\n",
    "            'NUM': 'numeral', 'PART': 'particle', 'PRON': 'pronoun',\n",
    "            'PROPN': 'proper noun', 'PUNCT': 'punctuation', 'X': 'other',\n",
    "            'SCONJ': 'subord conjunction', 'SYM': 'symbol', 'VERB': 'verb'}\n",
    "\"\"\"\n",
    "# create a dictionary to convert the pos tags to numbers, arbitrary\n",
    "pos_dict = {'NOUN' : 0, 'VERB' : 1, 'ADP' : 2, 'PROPN' : 3, 'PUNCT' : 4,\n",
    "            'DET' : 5, 'ADJ' : 6, 'NUM' : 7, 'ADV' : 8, 'PRON' : 9}\n",
    "other = len(pos_dict.values())  # fpr all other pos tags\n",
    "\n",
    "def convert_sentence_to_pos(sentence: str):\n",
    "  doc = nlp(sentence)\n",
    "  return [pos_dict.get(token.pos_, other) for token in doc]\n",
    "\n",
    "train_data['statement_pos'] = train_data['statement'].apply(convert_sentence_to_pos)\n",
    "valid_data['statement_pos'] = valid_data['statement'].apply(convert_sentence_to_pos)\n",
    "test_data['statement_pos'] = test_data['statement'].apply(convert_sentence_to_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all dependencies:\n",
    "dep_dict = {'ACL' : 0, 'ACOMP' : 1, 'ADVCL' : 2, 'ADVMOD' : 3, 'AGENT' : 4,\n",
    "            'AMOD' : 5, 'APPOS' : 6, 'ATTR' : 7, 'AUX' : 8, 'AUXPASS' : 9,\n",
    "            'CASE' : 10, 'CC' : 11, 'CCOMP' : 12, 'COMPOUND' : 13, 'CONJ' : 14,\n",
    "            'CSUBJ' : 15, 'CSUBJPASS' : 16, 'DATIVE' : 17, 'DEP' : 18,\n",
    "            'DET' : 19, 'DOBJ' : 20, 'EXPL' : 21, 'INTJ' : 22, 'MARK' : 23,\n",
    "            'META' : 24, 'NEG' : 25, 'NOUNMOD' : 26, 'NPMOD' : 27, 'NSUBJ' : 28,\n",
    "            'NSUBJPASS' : 29, 'NUMMOD' : 30, 'OPRD' : 31, 'PARATAXIS' : 32,\n",
    "            'PCOMP' : 33, 'POBJ' : 34, 'POSS' : 35, 'PRECONJ' : 36, 'PREDET' : 37,\n",
    "            'PREP' : 38, 'PRT' : 39, 'PUNCT' : 40, 'QUANTMOD' : 41,\n",
    "            'RELCL' : 42, 'ROOT' : 43, 'XCOMP' : 44}\n",
    "\"\"\"\n",
    "# create a dictionary to convert the dep tags to numbers, arbitrary\n",
    "dep_dict = {'punct' : 0, 'prep' : 1, 'pobj' : 2, 'compound' : 3, 'det' : 4,\n",
    "            'nsubj' : 5, 'ROOT' : 6, 'amod' : 7, 'dobj' : 8, 'aux' : 9}\n",
    "other = len(dep_dict.values())  # for all other dep tags\n",
    "\n",
    "def convert_sentence_to_dep(sentence):\n",
    "  doc = nlp(sentence)\n",
    "  return [dep_dict.get(token.dep_, other) for token in doc]\n",
    "\n",
    "train_data['statement_dep'] = train_data['statement'].apply(convert_sentence_to_dep)\n",
    "valid_data['statement_dep'] = valid_data['statement'].apply(convert_sentence_to_dep)\n",
    "test_data['statement_dep'] = test_data['statement'].apply(convert_sentence_to_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>barely-true</th>\n",
       "      <th>false</th>\n",
       "      <th>...</th>\n",
       "      <th>output</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>job_id</th>\n",
       "      <th>party_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>statement_freq</th>\n",
       "      <th>statement_pos</th>\n",
       "      <th>statement_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 5, 3, 3, 6, 0, 1, 6, 4, 0, 0, 2, 0, 4]</td>\n",
       "      <td>[6, 4, 10, 10, 7, 5, 10, 7, 0, 3, 8, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>[]</td>\n",
       "      <td>[10, 1, 5, 0, 2, 0, 0, 4, 9, 1, 10, 6, 0, 1, 2...</td>\n",
       "      <td>[10, 6, 4, 5, 1, 3, 2, 0, 5, 6, 10, 7, 5, 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3, 3, 1, 2, 3, 3, 4, 2, 1, 10, 1, 3, 3, 5, 0,...</td>\n",
       "      <td>[3, 5, 6, 1, 3, 2, 0, 1, 10, 9, 10, 3, 10, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 10, 6, 10, 1, 6, 0, 0, 0, 4]</td>\n",
       "      <td>[3, 3, 3, 5, 6, 10, 9, 10, 7, 3, 3, 8, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>[5, 6, 0, 1, 2, 5, 0, 2, 9, 0, 4]</td>\n",
       "      <td>[4, 7, 5, 6, 1, 4, 2, 1, 10, 2, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        label                                          statement  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state       party  barely-true  false  ...  output  speaker_id  job_id  \\\n",
       "0     Texas  republican          0.0    1.0  ...       1          25      19   \n",
       "1  Virginia    democrat          0.0    0.0  ...       3          25      25   \n",
       "2  Illinois    democrat         70.0   71.0  ...       4           0       0   \n",
       "3       NaN        none          7.0   19.0  ...       1          18      25   \n",
       "4   Florida    democrat         15.0    9.0  ...       3          14      25   \n",
       "\n",
       "  party_id  state_id  subject_id  venue_id  statement_freq  \\\n",
       "0        0         0          12        30              []   \n",
       "1        1         7          30        30              []   \n",
       "2        1         4          13        30              []   \n",
       "3        2        30           0         0              []   \n",
       "4        1         1           8        20              []   \n",
       "\n",
       "                                       statement_pos  \\\n",
       "0         [1, 5, 3, 3, 6, 0, 1, 6, 4, 0, 0, 2, 0, 4]   \n",
       "1  [10, 1, 5, 0, 2, 0, 0, 4, 9, 1, 10, 6, 0, 1, 2...   \n",
       "2  [3, 3, 1, 2, 3, 3, 4, 2, 1, 10, 1, 3, 3, 5, 0,...   \n",
       "3          [0, 0, 0, 0, 10, 6, 10, 1, 6, 0, 0, 0, 4]   \n",
       "4                  [5, 6, 0, 1, 2, 5, 0, 2, 9, 0, 4]   \n",
       "\n",
       "                                       statement_dep  \n",
       "0      [6, 4, 10, 10, 7, 5, 10, 7, 0, 3, 8, 1, 2, 0]  \n",
       "1  [10, 6, 4, 5, 1, 3, 2, 0, 5, 6, 10, 7, 5, 10, ...  \n",
       "2  [3, 5, 6, 1, 3, 2, 0, 1, 10, 9, 10, 3, 10, 4, ...  \n",
       "3          [3, 3, 3, 5, 6, 10, 9, 10, 7, 3, 3, 8, 0]  \n",
       "4                 [4, 7, 5, 6, 1, 4, 2, 1, 10, 2, 0]  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "We use the pretrained GloVe embeddings to convertwords into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 : Embeddings loaded\n",
      "100 : Embedding dimension\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "embeddings = {}\n",
    "word = ''\n",
    "try:\n",
    "  with open('glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0].lower()\n",
    "      embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "except FileNotFoundError:\n",
    "  print('File glove.6B.100d.txt was not found in this directory')\n",
    "  print('Get the file from the references provided in README.md')\n",
    "  raise FileNotFoundError\n",
    "print(len(embeddings), \": Embeddings loaded\")\n",
    "print(embed_dim, \": Embedding dimension\")\n",
    "\n",
    "num_words = len(vocab_dict) + 1\n",
    "embed_matrix = np.zeros((num_words, embed_dim))\n",
    "for word, i in vocab_dict.items():\n",
    "  embed_vector = embeddings.get(word)\n",
    "  if embed_vector is not None:\n",
    "    embed_matrix[i] = embed_vector\n",
    "\n",
    "pos_embeddings = np.identity(max(pos_dict.values()), dtype=int)\n",
    "dep_embeddings = np.identity(max(dep_dict.values()), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab_dict.keys())\n",
    "lstm_size = 100\n",
    "num_steps = 15\n",
    "num_epochs = 30\n",
    "batch_size = 40\n",
    "\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,3,3]\n",
    "filter_size = 128\n",
    "\n",
    "#Meta data related hyper params\n",
    "num_party = len(train_data.party_id.unique())\n",
    "num_state = len(train_data.state_id.unique())\n",
    "num_venue = len(train_data.venue_id.unique())\n",
    "num_job = len(train_data.job_id.unique())\n",
    "num_sub = len(train_data.subject_id.unique())\n",
    "num_speaker = len(train_data.speaker_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sentence Info (Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['statement_freq']\n",
    "X_val = valid_data['statement_freq']\n",
    "X_test = test_data['statement_freq']\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(train_data['output'], num_classes=6)\n",
    "Y_val = tf.keras.utils.to_categorical(valid_data['output'], num_classes=6)\n",
    "Y_test = list(test_data['output'])\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "X_train_pos = train_data['statement_pos']\n",
    "X_val_pos = valid_data['statement_pos']\n",
    "X_test_pos = test_data['statement_pos']\n",
    "\n",
    "X_train_pos = tf.keras.preprocessing.sequence.pad_sequences(X_train_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val_pos = tf.keras.preprocessing.sequence.pad_sequences(X_val_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test_pos = tf.keras.preprocessing.sequence.pad_sequences(X_test_pos, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "X_train_dep = train_data['statement_dep']\n",
    "X_val_dep = valid_data['statement_dep']\n",
    "X_test_dep = test_data['statement_dep']\n",
    "\n",
    "X_train_dep = tf.keras.preprocessing.sequence.pad_sequences(X_train_dep, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_val_dep = tf.keras.preprocessing.sequence.pad_sequences(X_val_dep, maxlen=num_steps, padding='post', truncating='post')\n",
    "X_test_dep = tf.keras.preprocessing.sequence.pad_sequences(X_test_dep, maxlen=num_steps, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_train = tf.keras.utils.to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "party_val = tf.keras.utils.to_categorical(valid_data['party_id'], num_classes=num_party)\n",
    "party_test = tf.keras.utils.to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "\n",
    "state_train = tf.keras.utils.to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "state_val = tf.keras.utils.to_categorical(valid_data['state_id'], num_classes=num_state)\n",
    "state_test = tf.keras.utils.to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "\n",
    "venue_train = tf.keras.utils.to_categorical(train_data['venue_id'], num_classes=num_venue)\n",
    "venue_val = tf.keras.utils.to_categorical(valid_data['venue_id'], num_classes=num_venue)\n",
    "venue_test = tf.keras.utils.to_categorical(test_data['venue_id'], num_classes=num_venue)\n",
    "\n",
    "job_train = tf.keras.utils.to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "job_val = tf.keras.utils.to_categorical(valid_data['job_id'], num_classes=num_job)\n",
    "job_test = tf.keras.utils.to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "\n",
    "subject_train = tf.keras.utils.to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "subject_val = tf.keras.utils.to_categorical(valid_data['subject_id'], num_classes=num_sub)\n",
    "subject_test = tf.keras.utils.to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "\n",
    "speaker_train = tf.keras.utils.to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "speaker_val = tf.keras.utils.to_categorical(valid_data['speaker_id'], num_classes=num_speaker)\n",
    "speaker_test = tf.keras.utils.to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "X_train_meta = np.hstack((party_train, state_train, venue_train, job_train, subject_train, speaker_train))\n",
    "X_val_meta = np.hstack((party_val, state_val, venue_val, job_val, subject_val, speaker_val))\n",
    "X_test_meta = np.hstack((party_test, state_test, venue_test, job_test, subject_test, speaker_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Matrix Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 155) (1284, 155) (1267, 155)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 6) (1284, 6)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_meta.shape, X_val_meta.shape, X_test_meta.shape)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_val.shape)\n",
    "print(X_train_pos.shape, X_val_pos.shape, X_test_pos.shape)\n",
    "print(X_train_dep.shape, X_val_dep.shape, X_test_dep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: tf.keras.models.Model, model_file_name: str, use_pos = False, use_meta = False, use_dep = False):\n",
    "  sgd = tf.keras.optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "  # adam = tf.keras.optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "  tb = tf.keras.callbacks.TensorBoard()\n",
    "  csv_logger = tf.keras.callbacks.CSVLogger('train.log')\n",
    "  filepath = model_file_name + '_weights.hdf5'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "  train_input = {'main_input': X_train}\n",
    "  valid_input = {'main_input': X_val}\n",
    "  if use_pos:\n",
    "    train_input['pos_input'] = X_train_pos\n",
    "    valid_input['pos_input'] = X_val_pos\n",
    "  if use_meta:\n",
    "    train_input['meta_input'] = X_train_meta\n",
    "    valid_input['meta_input'] = X_val_meta\n",
    "  if use_dep:\n",
    "    train_input['dep_input'] = X_train_dep\n",
    "    valid_input['dep_input'] = X_val_dep\n",
    "  if use_meta:\n",
    "    train_input['aux_input'] = X_train_meta\n",
    "    valid_input['aux_input'] = X_val_meta\n",
    "  model.fit(input, {'output': Y_train}, epochs=num_epochs, batch_size=batch_size, validation_data=(valid_input, {'main_output': Y_val}), callbacks=[tb, csv_logger, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this function\n",
    "def test(model_file_name: str, use_pos = False, use_meta = False, use_dep = False):\n",
    "  model: tf.keras.models.Model = tf.keras.models.load_model(model_file_name + '_weights.hdf5')\n",
    "  input = [X_test]\n",
    "  if use_pos:\n",
    "    input.append(X_test_pos)\n",
    "  if use_dep:\n",
    "    input.append(X_test_dep)\n",
    "  if use_meta:\n",
    "    input.append(X_test_meta)\n",
    "  predictions = model.predict(input, batch_size=batch_size, verbose=1)\n",
    "  n = len(predictions)\n",
    "\n",
    "  false_worst = [-1]*n\n",
    "  true_best = [-1]*n\n",
    "  for p in range(n):\n",
    "    if np.argmax(predictions[p]) == 0:\n",
    "      false_worst[p] = predictions[p][0]\n",
    "    elif np.argmax(predictions[p]) == 5:\n",
    "      true_best[p] = predictions[p][5]\n",
    "  print(n == len(Y_test))\n",
    "  correct = np.sum(np.argmax(predictions, axis=1) == Y_test, axis=1)\n",
    "  print(\"Correctly predicted: \", correct, \"out of\", n)\n",
    "  print(\"Accuracy: \", correct*100/n)\n",
    "  pickle.dump(predictions, open(model_file_name + '_predictions.pkl', 'wb'))\n",
    "\n",
    "  print(\"Printing the worst false predictions\")\n",
    "  for f in false_worst:\n",
    "    print(f[1])\n",
    "    print(test_data[f[0]])\n",
    "  print(\"Printing the best true predictions\")\n",
    "  for t in true_best:\n",
    "    print(t[1])\n",
    "    print(test_data[t[0]])\n",
    "  return false_worst, true_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pos = False\n",
    "use_meta = True\n",
    "use_dep = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/destrox/Project/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1732372049.784080    9318 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "statement_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x_stmt = tf.keras.layers.Embedding(vocab_length+1, embed_dim, weights=[embed_matrix], trainable=False)(statement_input)\n",
    "\n",
    "pos_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "x_pos = tf.keras.layers.Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], trainable=False)(pos_input)\n",
    "\n",
    "dep_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "x_dep = tf.keras.layers.Embedding(max(dep_dict.values()), max(dep_dict.values()), weights=[dep_embeddings], trainable=False)(dep_input)\n",
    "\n",
    "meta_input = tf.keras.layers.Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = tf.keras.layers.Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "kernel_stmt = []\n",
    "kernel_pos = []\n",
    "kernel_dep = []\n",
    "for kernel in kernel_sizes:\n",
    "  x_1 = tf.keras.layers.Conv1D(filter_size, kernel)(x_stmt)\n",
    "  x_1 = tf.keras.layer.GlobalMaxPooling1D()(x_1)\n",
    "  kernel_stmt.append(x_1)\n",
    "\n",
    "  x_2 = tf.keras.layers.Conv1D(filter_size, kernel)(x_pos)\n",
    "  x_2 = tf.keras.layers.GlobalMaxPooling1D()(x_2)\n",
    "  kernel_pos.append(x_2)\n",
    "\n",
    "  x_3 = tf.keras.layers.Conv1D(filter_size, kernel)(x_dep)\n",
    "  x_3 = tf.keras.layers.GlobalMaxPooling1D()(x_3)\n",
    "  kernel_dep.append(x_3)\n",
    "\n",
    "conv_in1 = tf.keras.layers.concatenate(kernel_stmt)\n",
    "conv_in1 = tf.keras.layers.Dropout(0.6)(conv_in1)\n",
    "conv_in1 = tf.keras.layers.Dense(128, activation='relu')(conv_in1)\n",
    "\n",
    "conv_in2 = tf.keras.layers.concatenate(kernel_pos)\n",
    "conv_in2 = tf.keras.layers.Dropout(0.6)(conv_in2)\n",
    "conv_in2 = tf.keras.layers.Dense(128, activation='relu')(conv_in2)\n",
    "\n",
    "conv_in3 = tf.keras.layers.concatenate(kernel_dep)\n",
    "conv_in3 = tf.keras.layers.Dropout(0.6)(conv_in3)\n",
    "conv_in3 = tf.keras.layers.Dense(128, activation='relu')(conv_in3)\n",
    "\n",
    "lays = [conv_in1]\n",
    "if use_pos:\n",
    "  lays.append(conv_in2)\n",
    "if use_dep:\n",
    "  lays.append(conv_in3)\n",
    "if use_meta:\n",
    "  lays.append(x_meta)\n",
    "x = tf.keras.layers.concatenate(lays)\n",
    "\n",
    "main_output = tf.keras.layers.Dense(6, activation='softmax', name='main_output')(x)\n",
    "inputs = [statement_input]\n",
    "if use_pos:\n",
    "  inputs.append(pos_input)\n",
    "if use_dep:\n",
    "  inputs.append(dep_input)\n",
    "if use_meta:\n",
    "  inputs.append(meta_input)\n",
    "model_cnn = tf.keras.models.Model(inputs=inputs, outputs=[main_output])\n",
    "print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m      2\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m      3\u001b[0m model_lstm\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_size))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model_lstm = tf.keras.models.Sequential()\n",
    "hidden_size = embed_dim\n",
    "model_lstm.add(tf.keras.layers.Embedding(vocab_length+1, hidden_size))\n",
    "model_lstm.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_size)))\n",
    "model_lstm.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "statement_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x_stmt = tf.keras.layers.Embedding(vocab_length+1, embed_dim, weights=[embed_matrix], trainable=False)(statement_input)\n",
    "lstm_in = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_stmt)\n",
    "\n",
    "pos_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "x_pos = tf.keras.layers.Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], trainable=False)(pos_input)\n",
    "lstm_in2 = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_pos)\n",
    "\n",
    "dep_input = tf.keras.layers.Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "x_dep = tf.keras.layers.Embedding(max(dep_dict.values()), max(dep_dict.values()), weights=[dep_embeddings], trainable=False)(dep_input)\n",
    "lstm_in3 = tf.keras.layers.LSTM(lstm_size, dropout=0.2)(x_dep)\n",
    "\n",
    "meta_input = tf.keras.layers.Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = tf.keras.layers.Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "lays = [lstm_in]\n",
    "if use_pos:\n",
    "  lays.append(lstm_in2)\n",
    "if use_dep:\n",
    "  lays.append(lstm_in3)\n",
    "if use_meta:\n",
    "  lays.append(x_meta)\n",
    "x = tf.keras.layers.concatenate(lays)\n",
    "\n",
    "main_output = tf.keras.layers.Dense(6, activation='softmax', name='main_output')(x)\n",
    "inputs = [statement_input]\n",
    "if use_pos:\n",
    "  inputs.append(pos_input)\n",
    "if use_dep:\n",
    "  inputs.append(dep_input)\n",
    "if use_meta:\n",
    "  inputs.append(meta_input)\n",
    "model_lstm = tf.keras.models.Model(inputs=inputs, outputs=[main_output])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_cnn, 'cnn', use_pos, use_meta, use_dep)\n",
    "test('cnn', use_pos, use_meta, use_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_lstm,'lstm', use_pos, use_meta, use_dep)\n",
    "test('lstm', use_pos, use_meta, use_dep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
